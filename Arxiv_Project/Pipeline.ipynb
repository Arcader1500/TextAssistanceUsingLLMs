{
 "cells": [
  {
   "cell_type": "code",
   "id": "a83167d28a72b063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T15:53:53.894741Z",
     "start_time": "2026-01-18T15:53:49.883920Z"
    }
   },
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Pipeline)\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "# Cleanup\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Configuration\n",
    "DB_PATH = os.path.join(os.getcwd(), \"arxiv\")  # Safe path joining\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d4d85b764a67dbe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T15:54:06.515240Z",
     "start_time": "2026-01-18T15:53:55.483239Z"
    }
   },
   "source": [
    "# Initialize Client & Embedding\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection = client.get_or_create_collection(name=\"Arxiv-Database\",\n",
    "                                             embedding_function=embedding_func)\n",
    "\n",
    "def search(query, top_k_retrieval=20, top_k_rerank=5):\n",
    "\n",
    "    # Stage 1: Semantic Retrieval (Bio-Encoder)\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k_retrieval\n",
    "    )\n",
    "    \n",
    "    documents = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "    \n",
    "    # Stage 2: Re-ranking (Cross-Encoder)\n",
    "    # Prepare pairs: (Query, Document_Context)\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    # Predict scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    \n",
    "    # Stage 3: Return top-k results\n",
    "    retrieved = []\n",
    "    for rank, idx in enumerate(ranked_indices[:top_k_rerank]):\n",
    "        retrieved.append({\n",
    "            \"rank\": rank+1,\n",
    "            \"score\": scores[idx],\n",
    "            \"source\": metadatas[idx]['source'],\n",
    "            \"page\": metadatas[idx]['page'],\n",
    "            \"content\": documents[idx]\n",
    "        })\n",
    "\n",
    "    return retrieved\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6cf7e96393738f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:31:01.271920Z",
     "start_time": "2026-01-18T11:30:32.606460Z"
    }
   },
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     quantization_config=bnb_config,\n",
    "#     # device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# )\n",
    "\n",
    "print(\"⏳ Loading Cross-Encoder Model...\")\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"✅ Cross-Encoder Loaded.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading Cross-Encoder Model...\n",
      "✅ Cross-Encoder Loaded.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b1f358ef76bac712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:48:17.984207Z",
     "start_time": "2026-01-18T11:48:17.506672Z"
    }
   },
   "source": [
    "query = \"What is the Assessment Misalignment problem in Large Language Models?\"\n",
    "docs = search(query)\n",
    "\n",
    "print(\"--- Retrieved Context ---\")\n",
    "for doc in docs:\n",
    "    print(f\"Content \\n: {doc['content']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Context ---\n",
      "Content \n",
      ": the reasoning ability of LLMs by alleviating the assessment misalignment problem caused by VFT.\n",
      "To address the assessment misalignment problem, in this paper, we propose an alignment fine-tuning\n",
      "(AFT) paradigm to improve LLM reasoning with three steps: 1) fine-tuning LLMs using COT\n",
      "training data; 2) generating multiple COT responses for each question using the fine-tuned LLMs,\n",
      "and categorizing them as positive and negative based on whether they deduce the correct answer;\n",
      "3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint\n",
      "alignment (CA) loss. Specifically, the CA loss ensures that all positive scores (the scores of positive\n",
      "COTs) are larger than negative scores. In addition, the negative scores are protected by a constraint\n",
      "term, which is proven to be very important in preventing model degradation. Beyond just binary\n",
      "positive and negative feedback, the CA loss can be seamlessly adapted to ranking situations when\n",
      "Content \n",
      ": which has been overlooked by these approaches, is also crucial for their performance. Extensive\n",
      "experiments on four reasoning benchmarks demonstrate the effectiveness of AFT. In addition, AFT\n",
      "also performs well in multi-task and out-of-distribution situations.\n",
      "8 L IMITATIONS\n",
      "Our paper has some limitations, which should be discussed in future works: 1) Due to the resource\n",
      "limit, we do not scale the AFT to larger LLMs such as 65B and 70B LLama models. However, we\n",
      "believe that larger models still suffer from the assessment misalignment problem of VFT, and thus\n",
      "AFT can improve the performance of these larger models; 2) Our boundary constraint alignment loss\n",
      "incorporates a hyper-parameter β that regulates the constraint strength, significantly impacting the\n",
      "model’s performance. Finding the optimal hyper-parameter requires constructing a validation set\n",
      "and a certain search overhead. Although our detached alignment loss can mitigate the assessment\n",
      "Content \n",
      ": fine-tuned LLMs suffer from an Assessment Misalignment problem, i.e., they\n",
      "frequently assign higher scores to subpar COTs, leading to potential limitations\n",
      "in their reasoning abilities. To address this problem, we introduce an Alignment\n",
      "Fine-Tuning (AFT)paradigm, which involves three steps: 1) fine-tuning LLMs with\n",
      "COT training data; 2) generating multiple COT responses for each question, and\n",
      "categorizing them into positive and negative ones based on whether they achieve\n",
      "the correct answer; 3) calibrating the scores of positive and negative responses\n",
      "given by LLMs with a novel constraint alignment loss. Specifically, the constraint\n",
      "alignment loss has two objectives: a) Alignment, which guarantees that positive\n",
      "scores surpass negative scores to encourage answers with high-quality COTs;\n",
      "b) Constraint, which keeps the negative scores confined to a reasonable range\n",
      "to prevent the model degradation. Beyond just the binary positive and negative\n",
      "Content \n",
      ": Preprint\n",
      "indicate that AFT not only improves the performance of in-distribution tasks but also enhances the\n",
      "model’s transfer ability, leading to significantly better out-of-distribution performance.\n",
      "7 C ONCLUSION\n",
      "In this paper, we find that the vanilla fine-tuned (VFT) LLMs with chain-of-thought (COT) reasoning\n",
      "process suffer from an assessment misalignment problem, i.e, they fail to access the quality of different\n",
      "COTs of the learned questions, which hinders the reasoning ability of LLMs. To this end, we propose\n",
      "an alignment fine-tuning (AFT) paradigm. Our AFT consists of a novel constraint alignment loss that\n",
      "can align the model assessment behaviors without harming the model performance. Furthermore, we\n",
      "also delve deeply into recent widely used ranking losses for alignment and find that the constraint,\n",
      "which has been overlooked by these approaches, is also crucial for their performance. Extensive\n",
      "Content \n",
      ": correct answer. In this paper, we find that previous vanilla fine-tuning (VFT) paradigm causes LLMs\n",
      "to suffer from an Assessment Misalignment problem, i.e., LLMs struggle with accessing the quality\n",
      "1\n",
      "arXiv:2309.02144v1  [cs.CL]  5 Sep 2023\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "af40df56bf198a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:52:56.685382Z",
     "start_time": "2026-01-18T11:52:56.667187Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "\n",
    "class RAGGenerator:\n",
    "    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = \"gpt-5-nano:free\"):\n",
    "        \"\"\"\n",
    "        Initialize the OpenAI-compatible client.\n",
    "\n",
    "        Args:\n",
    "            api_key: Your API Key (use \"dummy\" for local models like Ollama)\n",
    "            base_url: The API endpoint (e.g., \"http://localhost:11434/v1\" for Ollama)\n",
    "            model_name: The specific model to target (e.g., \"llama3\", \"gpt-4o\")\n",
    "        \"\"\"\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key or os.getenv(\"OPENAI_API_KEY\"),\n",
    "            base_url=base_url or os.getenv(\"OPENAI_BASE_URL\")\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def construct_prompt(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Builds the prompt by combining the user query with retrieved context.\n",
    "        \"\"\"\n",
    "        # Join chunks with a clear separator\n",
    "        context_str = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "\n",
    "        prompt = f\"\"\"You are a helpful assistant for maritime regulations.\n",
    "Answer the user's question based ONLY on the following context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "The context may contain Markdown tables. Please interpret the rows and columns accurately.\n",
    "\n",
    "### CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "### USER QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_answer(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Sends the prompt to the LLM and returns the response.\n",
    "        \"\"\"\n",
    "        prompt = self.construct_prompt(query, context_chunks)\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a precise technical assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1, # Keep strict for RAG to avoid hallucinations\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error during inference: {e}\"\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:53:00.444088Z",
     "start_time": "2026-01-18T11:53:00.396328Z"
    }
   },
   "cell_type": "code",
   "source": "rag = RAGGenerator()",
   "id": "23a4a34ab0059bbe",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T12:02:52.227126Z",
     "start_time": "2026-01-18T12:02:22.374897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. SIMULATE RETRIEVAL\n",
    "query = \"How does the 'Constraint' objective in AFT prevent model degradation?\"\n",
    "docs = search(query)\n",
    "context = [\"-> \".join([doc['source'], doc['content']]) for doc in docs]\n",
    "# 2. GENERATE RESPONSE\n",
    "print(f\"Query: {query}\\n\")\n",
    "answer = rag.generate_answer(query, context)\n",
    "print(f\"Response:\\n{answer}\")"
   ],
   "id": "b6b3aa4b1bd318ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the 'Constraint' objective in AFT prevent model degradation?\n",
      "\n",
      "Response:\n",
      "The Constraint objective keeps negative scores confined to a reasonable range using a constraint term in the CA loss. This bound prevents negative scores from drifting too far, which protects against deterioration of the model’s performance (i.e., prevents model degradation) while still ensuring positive scores exceed negatives.\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
