{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: LLM Basics & Inference (Low VRAM)\n",
    "\n",
    "**Goal**: Load a \"smart\" LLM heavily compressed (4-bit) so it fits in your 4GB GPU, and talk to it.\n",
    "\n",
    "**The Model**: `Qwen/Qwen2.5-1.5B-Instruct`. \n",
    "Why this model? It's one of the best \"tiny\" models available right now. It outperforms many larger old models and is perfect for learning.\n",
    "\n",
    "**Technique**: \n",
    "- **Quantization**: We represent numbers with fewer bits (4 instead of 16). This reduces VRAM usage by ~70% with minimal quality loss.\n",
    "- **Library**: We use `bitsandbytes` to handle this on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:33:15.882147Z",
     "start_time": "2025-12-24T05:33:15.630505Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# Force garbage collection to clear any old memory if you're rerunning cells\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure 4-bit Quantization\n",
    "This is the magic part. We create a config that tells `transformers` to load the model in 4-bit strict NF4 format (Normal Float 4) to save maximum memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:33:19.735140Z",
     "start_time": "2025-12-24T05:33:19.718675Z"
    }
   },
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,      # Quantize the quantization constants (extra space saving)\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Normal Float 4 is better for LLM weights\n",
    "    bnb_4bit_compute_dtype=torch.float16 # Compute in 16-bit for speed\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Model\n",
    "This might take a minute to download (~3GB)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:36:12.892115Z",
     "start_time": "2025-12-24T05:33:22.839928Z"
    }
   },
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "print(\"Loading model... this controls the VRAM usage.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Automatically puts layers on GPU\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... this controls the VRAM usage.\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference (Chat)\n",
    "Let's ask it a question."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:41:12.429375Z",
     "start_time": "2025-12-24T05:40:26.224225Z"
    }
   },
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to check for prime numbers.\"}\n",
    "]\n",
    "\n",
    "# Format the prompt specifically for Qwen (ChatML format)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "# Decode only the new tokens\n",
    "generated_ids = [ \n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an example Python function that checks if a number is prime:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    # Check if n is less than 2 (not prime)\n",
      "    if n < 2:\n",
      "        return False\n",
      "    \n",
      "    # Iterate from 2 to the square root of n\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        # If n is divisible by any number between 2 and its square root,\n",
      "        # it's not prime\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    \n",
      "    # If we've gone through all possible divisors without finding one,\n",
      "    # n must be prime\n",
      "    return True\n",
      "```\n",
      "\n",
      "This function takes an integer `n` as input and returns `True` if `n` is prime, or `False` otherwise.\n",
      "\n",
      "Here's how you can use this function:\n",
      "\n",
      "```python\n",
      "# Example usage\n",
      "print(is_prime(7))   # Output: True\n",
      "print(is_prime(4))   # Output: False\n",
      "print(is_prime(9))   # Output: False\n",
      "```\n",
      "\n",
      "Note that this implementation uses a basic primality test based on trial division. It may not perform well for very large numbers due to time complexity constraints. There are more efficient algorithms like the Miller-Rabin primality test available.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Check\n",
    "Let's see how much memory we used. It should be well under 4GB."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:41:17.861274Z",
     "start_time": "2025-12-24T05:41:17.846851Z"
    }
   },
   "source": [
    "print(f\"Updated VRAM usage: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated VRAM usage: 1.17 GB\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
