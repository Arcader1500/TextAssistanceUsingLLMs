{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Fine-Tuning with LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**Goal**: Train the model to output a specific JSON format.\n",
    "\n",
    "**Fix applied**: Explicitly disabled BFloat16 and forced model loading in Float16 to prevent Windows AMP errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:09:20.654123Z",
     "start_time": "2025-12-24T06:09:04.321681Z"
    }
   },
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:09:22.758377Z",
     "start_time": "2025-12-24T06:09:22.734684Z"
    }
   },
   "source": [
    "data = [\n",
    "    {\"text\": \"User: I loved this movie! \\nAssistant: {\\\"sentiment\\\": \\\"positive\\\", \\\"score\\\": 10}\"},\n",
    "    {\"text\": \"User: It was terrible and boring. \\nAssistant: {\\\"sentiment\\\": \\\"negative\\\", \\\"score\\\": 1}\"},\n",
    "    {\"text\": \"User: The food was okay, but service was slow. \\nAssistant: {\\\"sentiment\\\": \\\"neutral\\\", \\\"score\\\": 5}\"},\n",
    "    {\"text\": \"User: Best day ever! \\nAssistant: {\\\"sentiment\\\": \\\"positive\\\", \\\"score\\\": 10}\"},\n",
    "    {\"text\": \"User: I hate rain. \\nAssistant: {\\\"sentiment\\\": \\\"negative\\\", \\\"score\\\": 2}\"},\n",
    "    {\"text\": \"User: It was a disaster. \\nAssistant: {\\\"sentiment\\\": \\\"negative\\\", \\\"score\\\": 0}\"},\n",
    "    {\"text\": \"User: Absolutely fantastic service. \\nAssistant: {\\\"sentiment\\\": \\\"positive\\\", \\\"score\\\": 9}\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "print(\"Dataset ready:\", dataset[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: {'text': 'User: I loved this movie! \\nAssistant: {\"sentiment\": \"positive\", \"score\": 10}'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model (Force Float16)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:09:48.357966Z",
     "start_time": "2025-12-24T06:09:25.222288Z"
    }
   },
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# 1. Configure Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 # Compute in FP16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load Model with explicit torch_dtype=float16\n",
    "# This ensures the base model config doesn't default to BFloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16 # <--- CRITICAL FIX: Force FP16\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:09:54.104254Z",
     "start_time": "2025-12-24T06:09:53.825588Z"
    }
   },
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 1,544,803,840 || trainable%: 0.0705\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training (Strict FP16)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:10:10.510123Z",
     "start_time": "2025-12-24T06:09:58.076019Z"
    }
   },
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../qwen_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    max_steps=10,\n",
    "    fp16=True,                 # Enable FP16\n",
    "    bf16=False,                # <--- CRITICAL FIX: Explicitly disable BF16\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_length=512,        # Constrain sequence length\n",
    "    dataset_text_field=\"text\"   # Memory efficient optimizer\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args  # Explicitly specify text column\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffdc6949539e461f902a2fb0e4345534"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b98450cedc4441628d23dd7fb0f0a243"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c8962485d334060bb9551d3e0c68d9f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotImplementedError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     17\u001B[39m trainer = SFTTrainer(\n\u001B[32m     18\u001B[39m     model=model,\n\u001B[32m     19\u001B[39m     train_dataset=dataset,\n\u001B[32m     20\u001B[39m     args=training_args  \u001B[38;5;66;03m# Explicitly specify text column\u001B[39;00m\n\u001B[32m     21\u001B[39m )\n\u001B[32m     23\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining complete!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/transformers/trainer.py:2715\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2713\u001B[39m         grad_norm_context = implicit_replication\n\u001B[32m   2714\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m grad_norm_context():\n\u001B[32m-> \u001B[39m\u001B[32m2715\u001B[39m         _grad_norm = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclip_grad_norm_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2716\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2717\u001B[39m \u001B[43m            \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2718\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2720\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2721\u001B[39m     is_accelerate_available()\n\u001B[32m   2722\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED\n\u001B[32m   2723\u001B[39m ):\n\u001B[32m   2724\u001B[39m     grad_norm = model.get_global_grad_norm()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/accelerate/accelerator.py:3008\u001B[39m, in \u001B[36mAccelerator.clip_grad_norm_\u001B[39m\u001B[34m(self, parameters, max_norm, norm_type)\u001B[39m\n\u001B[32m   3006\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m parameters == [p \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m model.parameters()]:\n\u001B[32m   3007\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m model.clip_grad_norm_(max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m3008\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43munscale_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3009\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=norm_type)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/accelerate/accelerator.py:2946\u001B[39m, in \u001B[36mAccelerator.unscale_gradients\u001B[39m\u001B[34m(self, optimizer)\u001B[39m\n\u001B[32m   2944\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(opt, AcceleratedOptimizer):\n\u001B[32m   2945\u001B[39m     opt = opt.optimizer\n\u001B[32m-> \u001B[39m\u001B[32m2946\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43munscale_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/torch/amp/grad_scaler.py:343\u001B[39m, in \u001B[36mGradScaler.unscale_\u001B[39m\u001B[34m(self, optimizer)\u001B[39m\n\u001B[32m    336\u001B[39m inv_scale = (\n\u001B[32m    337\u001B[39m     \u001B[38;5;28mself\u001B[39m._scale.double().reciprocal().float()\n\u001B[32m    338\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._scale.device != torch.device(\u001B[33m\"\u001B[39m\u001B[33mmps:0\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    339\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._scale.reciprocal()\n\u001B[32m    340\u001B[39m )\n\u001B[32m    341\u001B[39m found_inf = torch.full((), \u001B[32m0.0\u001B[39m, dtype=torch.float32, device=\u001B[38;5;28mself\u001B[39m._scale.device)\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m optimizer_state[\u001B[33m\"\u001B[39m\u001B[33mfound_inf_per_device\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_unscale_grads_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minv_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m    345\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    346\u001B[39m optimizer_state[\u001B[33m\"\u001B[39m\u001B[33mstage\u001B[39m\u001B[33m\"\u001B[39m] = OptState.UNSCALED\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/RAG/lib/python3.12/site-packages/torch/amp/grad_scaler.py:280\u001B[39m, in \u001B[36mGradScaler._unscale_grads_\u001B[39m\u001B[34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001B[39m\n\u001B[32m    278\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m device, per_dtype_grads \u001B[38;5;129;01min\u001B[39;00m per_device_and_dtype_grads.items():\n\u001B[32m    279\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m grads \u001B[38;5;129;01min\u001B[39;00m per_dtype_grads.values():\n\u001B[32m--> \u001B[39m\u001B[32m280\u001B[39m             \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_amp_foreach_non_finite_check_and_unscale_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    281\u001B[39m \u001B[43m                \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    282\u001B[39m \u001B[43m                \u001B[49m\u001B[43mper_device_found_inf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[43m                \u001B[49m\u001B[43mper_device_inv_scale\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    284\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m per_device_found_inf._per_device_tensors\n",
      "\u001B[31mNotImplementedError\u001B[39m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"final_adapter\")\n",
    "print(\"Adapter saved to 'final_adapter' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
